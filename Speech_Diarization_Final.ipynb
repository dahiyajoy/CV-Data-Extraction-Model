{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dahiyajoy/CV-Data-Extraction-Model/blob/Master/Speech_Diarization_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGRTyCfIb2HF"
      },
      "source": [
        "# **Speech Diarizartion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IxYITgmGfR9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b67253-7d54-4fbd-b08f-5f81450d8e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/Audio Trimmed.mp3'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "buGt4moR5Mac"
      },
      "outputs": [],
      "source": [
        "num_speakers = 2\n",
        "\n",
        "language = 'English'\n",
        "\n",
        "model_size = 'large'\n",
        "\n",
        "model_name = model_size\n",
        "if language == 'English' and model_size != 'large':\n",
        "  model_name += '.en'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "O0_tup8RAyBy"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git > /dev/null\n",
        "!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null\n",
        "\n",
        "import whisper\n",
        "import datetime\n",
        "\n",
        "import subprocess\n",
        "\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dlg-jlgEPyTT"
      },
      "outputs": [],
      "source": [
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "\n",
        "# Try to use the GPU, if available and driver is correctly installed. Otherwise, default to CPU.\n",
        "# try:\n",
        "#   embedding_model = PretrainedSpeakerEmbedding(\n",
        "#       \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "#       device=torch.device(\"cuda\"))\n",
        "# except RuntimeError:\n",
        "#   print(\"No GPU found or driver issue. Defaulting to CPU.\")\n",
        "embedding_model = PretrainedSpeakerEmbedding(\n",
        "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    device=torch.device(\"cpu\"))\n",
        "\n",
        "\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DiE3hs3jnTlf"
      },
      "outputs": [],
      "source": [
        "if file_path[-3:] != 'wav':\n",
        "  subprocess.call(['ffmpeg', '-i', file_path, 'audio.wav', '-y'])\n",
        "  file_path = 'audio.wav'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Vdbad9x5CHkC"
      },
      "outputs": [],
      "source": [
        "model = whisper.load_model(model_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z4uw8CrovIN1"
      },
      "outputs": [],
      "source": [
        "result = model.transcribe(file_path)\n",
        "segments = result[\"segments\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U1sZYZ_pkV1u"
      },
      "outputs": [],
      "source": [
        "with contextlib.closing(wave.open(file_path,'r')) as f:\n",
        "  frames = f.getnframes()\n",
        "  rate = f.getframerate()\n",
        "  duration = frames / float(rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "i9R5bpc3_EOL"
      },
      "outputs": [],
      "source": [
        "audio = Audio()\n",
        "\n",
        "def segment_embedding(segment):\n",
        "  start = segment[\"start\"]\n",
        "  # Whisper overshoots the end timestamp in the last segment\n",
        "  end = min(duration, segment[\"end\"])\n",
        "  clip = Segment(start, end)\n",
        "  waveform, sample_rate = audio.crop(file_path, clip)\n",
        "  return embedding_model(waveform[None])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hH1wVl66rcHb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "audio = Audio()\n",
        "\n",
        "def segment_embedding(segment):\n",
        "    start = segment[\"start\"]\n",
        "    # Whisper overshoots the end timestamp in the last segment\n",
        "    end = min(duration, segment[\"end\"])\n",
        "    clip = Segment(start, end)\n",
        "    waveform, sample_rate = audio.crop(file_path, clip)\n",
        "\n",
        "    # Ensure waveform is mono\n",
        "    if waveform.ndim > 1 and waveform.shape[0] > 1:\n",
        "        # Convert to mono by averaging channels using PyTorch\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    return embedding_model(waveform[None])\n",
        "\n",
        "embeddings = np.zeros(shape=(len(segments), 192))\n",
        "for i, segment in enumerate(segments):\n",
        "    embeddings[i] = segment_embedding(segment)\n",
        "\n",
        "embeddings = np.nan_to_num(embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QHvbUf8sgUVA"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import datetime\n",
        "\n",
        "# Assuming embeddings and segments are already defined\n",
        "clustering = AgglomerativeClustering(n_clusters=num_speakers).fit(embeddings)\n",
        "labels = clustering.labels_\n",
        "\n",
        "# Map cluster labels to speaker names\n",
        "speaker_names = {0: 'Curtis', 1: 'Amy'}\n",
        "for i in range(len(segments)):\n",
        "    segments[i][\"speaker\"] = speaker_names.get(labels[i], 'Unknown')\n",
        "\n",
        "def time(secs):\n",
        "    return datetime.timedelta(seconds=round(secs))\n",
        "\n",
        "with open(\"transcript.txt\", \"w\") as f:\n",
        "    for i, segment in enumerate(segments):\n",
        "        if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "            f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n')\n",
        "        f.write(segment[\"text\"][1:] + ' ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrGdsNDUud82",
        "outputId": "6f0fa660-e069-49ca-fc92-045cf5c7b4b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Curtis 0:00:00\n",
            "Hi everybody, it's Curtis and I'm here with Amy from Real English Conversations and we're going to talk about driving in Canada and North America. And it all starts with getting your learner's license, right Amy? \n",
            "Amy 0:00:17\n",
            "Yeah, definitely. In order to be on the road behind the wheel of a vehicle, you've got to have a license. And the first thing that you do is you actually, at least in Canada, we do things in a very specific way. We have something called a graduated licensing program. I'm sure that the United States does it differently. But you pick up this book and it has all the rules of the road. \n",
            "Curtis 0:00:45\n",
            "Yeah, and you have to study it because you have to write a written test or something like that. \n",
            "Amy 0:00:52\n",
            "Yeah, it's been a long time, right Curtis? \n",
            "Curtis 0:00:54\n",
            "It's been a very long time for me, yeah. \n",
            "Amy 0:01:00\n",
            "So, yeah, you have to go in. There's a test that makes sure that obviously you've studied the book, you know about the rules of making a right-hand turn, for example, when you need to yield, when you need to stop, all of those sort of things. \n",
            "Curtis 0:01:19\n",
            "Parallel parking? \n",
            "Amy 0:01:20\n",
            "Yep, and then if you pass the test, they give you a license. They give you a license which you're able to drive a vehicle under the supervision of someone who has a proper license. \n",
            "Curtis 0:01:32\n",
            "Right. \n",
            "Amy 0:01:33\n",
            "Anyway, we have in Canada, there's a phase where after you've had your training with whoever is teaching you, you come back and you get another license which has restrictions for a year or so until you pass your final exam and you're able to drive without all of the rules. \n",
            "Curtis 0:01:52\n",
            "Is that your novice license? \n",
            "Amy 0:01:54\n",
            "Yeah, we call it the novice license period or something. I'm not sure the exact terminology. \n"
          ]
        }
      ],
      "source": [
        "# Load the contents of the .txt file\n",
        "with open('transcript.txt', 'r', encoding='utf-8') as file:\n",
        "    file_content = file.read()\n",
        "\n",
        "print(file_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O6Xkb6ab769"
      },
      "source": [
        "# **Speech Summarization**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZmpgEJGcLlr",
        "outputId": "4938ece2-a4f8-473a-b493-3f7f8061798c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsC6HSBAcLow",
        "outputId": "4bb8d2ab-6097-41fd-9a81-9297be90ddd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of the Conversation:\n",
            "  In Canada, at least in Canada, we do things in a very specific way. We have something called a graduated licensing program. I'm sure that the United States does it differently. But you pick up this book and it has all the rules of the road. You have to study it because you have to write a written test .\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the summarizer pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Load the contents of the .txt file\n",
        "with open('transcript.txt', 'r') as file:\n",
        "    file_content = file.read()\n",
        "\n",
        "# Remove occurrences of \"SPEAKER 1\", \"SPEAKER 2\", \"SPEAKER 3\"\n",
        "# content_cleaned = file_content.replace('SPEAKER 1', '').replace('SPEAKER 2', '').replace('SPEAKER 3', '')\n",
        "\n",
        "# Generate the summary\n",
        "summary = summarizer(file_content, max_length=150, min_length=40, do_sample=False)\n",
        "\n",
        "# Save the summary to a new file\n",
        "with open('summary.txt', 'w', encoding='utf-8') as summary_file:\n",
        "    summary_file.write(summary[0]['summary_text'])\n",
        "\n",
        "# Print the summary\n",
        "print(\"\\nSummary of the Conversation:\\n\", summary[0]['summary_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XJkZXbNcBVe"
      },
      "source": [
        "# **Speech Translation to Arabic**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QQSgfYCccD4",
        "outputId": "5182abf0-894b-4674-8d41-f5d0b4694ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.7.4)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIp_pZfKeCiO"
      },
      "source": [
        "## **Summary Translation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6is51xncem3",
        "outputId": "f6cd175e-5285-4f79-c1bd-25e59dbf0762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Summary:\n",
            "  In Canada, at least in Canada, we do things in a very specific way. We have something called a graduated licensing program. I'm sure that the United States does it differently. But you pick up this book and it has all the rules of the road. You have to study it because you have to write a written test .\n",
            "\n",
            "Translated Summary:\n",
            " في كندا ، على الأقل في كندا ، نفعل الأشياء بطريقة محددة للغاية.لدينا شيء يسمى برنامج الترخيص المتدرج.أنا متأكد من أن الولايات المتحدة تفعل ذلك بشكل مختلف.لكنك تلتقط هذا الكتاب ولديه جميع قواعد الطريق.عليك أن تدرسها لأنه يجب عليك كتابة اختبار كتابي.\n"
          ]
        }
      ],
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "# Initialize the Translator\n",
        "translator = Translator()\n",
        "\n",
        "# Load the contents of the .txt file\n",
        "with open('summary.txt', 'r', encoding='utf-8') as file:\n",
        "    file_content = file.read()\n",
        "\n",
        "# Translate the content to Arabic\n",
        "translated = translator.translate(file_content, dest='ar')\n",
        "\n",
        "# Save the Translated Transcripts to a new file\n",
        "with open('summary_arabic.txt', 'w', encoding='utf-8') as output_file2:\n",
        "    output_file2.write(translated.text)  # Use translated.text to get the string\n",
        "\n",
        "# Print the original and translated content (optional)\n",
        "print(\"Original Summary:\\n\", file_content)\n",
        "print(\"\\nTranslated Summary:\\n\", translated.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66a3aDeBeHAY"
      },
      "source": [
        "## **Transcript Translation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJfQ_NAveK8e",
        "outputId": "27dadbf4-98c3-49fc-f941-82b5af5fb3df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translated Trnascripts:\n",
            " كورتيس 0:00:00\n",
            "مرحباً بالجميع ، إنه كورتيس وأنا هنا مع إيمي من محادثات اللغة الإنجليزية الحقيقية وسنتحدث عن القيادة في كندا وأمريكا الشمالية.ويبدأ كل شيء بالحصول على رخصة المتعلم ، أليس كذلك؟\n",
            "ايمي 0:00:17\n",
            "نعم بالتأكيد.لكي تكون على الطريق خلف عجلة السيارة ، يجب أن يكون لديك ترخيص.وأول ما تفعله هو أنك في الواقع ، على الأقل في كندا ، نفعل الأشياء بطريقة محددة للغاية.لدينا شيء يسمى برنامج الترخيص المتدرج.أنا متأكد من أن الولايات المتحدة تفعل ذلك بشكل مختلف.لكنك تلتقط هذا الكتاب ولديه جميع قواعد الطريق.\n",
            "كورتيس 0:00:45\n",
            "نعم ، وعليك أن تدرسه لأنك يجب أن تكتب اختبارًا مكتوبًا أو شيء من هذا القبيل.\n",
            "ايمي 0:00:52\n",
            "نعم ، لقد مر وقت طويل ، كورتيس صحيح؟\n",
            "كورتيس 0:00:54\n",
            "لقد مر وقت طويل جدًا بالنسبة لي ، نعم.\n",
            "ايمي 0:01:00\n",
            "لذا ، نعم ، عليك أن تدخل. هناك اختبار يتأكد من الواضح أنك درست الكتاب ، أنت تعرف عن قواعد جعل منعطفًا يمينيًا ، على سبيل المثال ، عندما تحتاج إلى العائد ، عندما تحتاجللتوقف ، كل هذا النوع من الأشياء.\n",
            "كورتيس 0:01:19\n",
            "موازنة موازنة؟\n",
            "ايمي 0:01:20\n",
            "نعم ، ثم إذا اجتازت الاختبار ، فإنها تعطيك ترخيصًا.يعطونك ترخيصًا قادرًا على قيادة سيارة تحت إشراف شخص لديه ترخيص مناسب.\n",
            "كورتيس 0:01:32\n",
            "يمين.\n",
            "ايمي 0:01:33\n",
            "على أي حال ، لدينا في كندا ، هناك مرحلة حيث بعد أن تدربك مع من يعلمك ، ستعود وستحصل على ترخيص آخر له قيود لمدة عام أو نحو ذلك حتى تجتاز الاختبار النهائي وأنتقادرة على القيادة دون كل القواعد.\n",
            "كورتيس 0:01:52\n",
            "هل هذا ترخيص المبتدئ الخاص بك؟\n",
            "ايمي 0:01:54\n",
            "نعم ، نسميها فترة ترخيص المبتدئ أو شيء من هذا القبيل.لست متأكدًا من المصطلحات الدقيقة.\n"
          ]
        }
      ],
      "source": [
        "from googletrans import Translator\n",
        "\n",
        "# Initialize the Translator\n",
        "translator = Translator()\n",
        "\n",
        "# Load the contents of the .txt file\n",
        "with open('transcript.txt', 'r', encoding='utf-8') as file:\n",
        "    file_content = file.read()\n",
        "\n",
        "# Translate the content to Arabic\n",
        "translated = translator.translate(file_content, dest='ar')\n",
        "\n",
        "# Save the Translated Transcripts to a new file\n",
        "with open('transcript_arabic.txt', 'w', encoding='utf-8') as output_file2:\n",
        "    output_file2.write(translated.text)  # Use translated.text to get the string\n",
        "\n",
        "# Print the original and translated content (optional)\n",
        "print(\"\\nTranslated Trnascripts:\\n\", translated.text)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}